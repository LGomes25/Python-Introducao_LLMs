# ğŸ“– IntroduÃ§Ã£o Ã s LLMs
Este repositÃ³rio reÃºne os estudos e implementaÃ§Ãµes realizados em uma trilha de introduÃ§Ã£o a Large Language Models (LLMs).
O projeto estÃ¡ organizado em mÃ³dulos e aulas, mostrando a evoluÃ§Ã£o desde chamadas simples de modelos atÃ© a construÃ§Ã£o de um chatbot completo com Streamlit, LangChain e LangGraph.

# ğŸš€ Estrutura do Projeto
A pasta src/ contÃ©m os cÃ³digos separados por mÃ³dulos e aulas:

- MÃ³dulo 2 â€“ Terminal
- m2aula1: IntroduÃ§Ã£o a LLM com chamada direta e resposta.
- m2aula2: Uso do LangChain com messages e prompts.
- m2aula3: IntroduÃ§Ã£o ao LangGraph e suas estruturas.
- m2aula4: Leitura de PDF e integraÃ§Ã£o com LangChain/LangGraph.
- MÃ³dulo 3 â€“ Streamlit
- m3aula1: Primeiros passos com Streamlit.
- m3aula2: Backend com LangChain/LangGraph e frontend com Streamlit.
- m3aula3: Leitura de PDF integrada ao frontend/backend.
- m3aula4: Uso de streaming para mostrar a formaÃ§Ã£o da resposta do LLM.
- Extra: Bot com histÃ³rico como contexto, permitindo memÃ³ria de conversa.

# âš™ï¸ ConfiguraÃ§Ã£o do LM Studio
Este projeto utiliza o LM Studio como servidor local de modelos.
No arquivo config.py, estÃ£o definidas funÃ§Ãµes para configurar chamadas tanto via LangChain quanto via cliente OpenAI:

- Base URL: http://127.0.0.1:1234/v1
- API Key: lm-studio
- Modelo: meta-llama-3.1-8b-instruct
  FunÃ§Ãµes disponÃ­veis:
- get_langchain_model(): retorna o modelo configurado para uso com LangChain.
- get_openai_client(): retorna o cliente OpenAI para chamadas diretas.
  Isso permite alternar facilmente entre chamadas via LangChain e chamadas diretas ao servidor do LM Studio.

# âš™ï¸ Criando um ambiente
Utilize os comandos no terminal, dentro da raiz do projeto.
```text
- python -m venv .venv
```

# ğŸ–¥ï¸ Selecionando o interpretador
- Ctrl + Shift + P
- Digite: Python: Select Interpreter
- Escolha o Python dentro de .venv
  Se nÃ£o for criada automaticamente a pasta .vscode/settings.json, crie manualmente com:
  ```text
  {
    "python-envs.pythonProjects": [],
    "python.formatting.provider": "black",
    "editor.formatOnSave": true,
    "python.analysis.typeCheckingMode": "basic",
    "python.defaultInterpreterPath": ".venv/Scripts/python.exe"
  }
  ```

# ğŸ“‚ Estrutura de Pastas
```text
â”œâ”€â”€ src/         # CÃ³digo principal (mÃ³dulos e aulas)
â”œâ”€â”€ data/        # PDFs e arquivos de suporte
â”œâ”€â”€ .venv/       # Ambiente virtual
â”œâ”€â”€ .vscode/     # ConfiguraÃ§Ãµes do VS Code
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md    # DocumentaÃ§Ã£o
```

# ğŸ“¦ InstalaÃ§Ã£o das dependÃªncias

Atualize o pip e instale as bibliotecas:
```text
- python.exe -m pip install --upgrade pip

- pip list                             // verificar a lista do pip

- pip install -U langchain-core
- pip install -U langchain-community
- pip install -U langchain-openai
- pip install -U langchain-tools       // para decorators tipo @tools
- pip install -U langgraph
- pip install -U pypdf                 // leitor de pdf
- pip install -U pymupdf               // leitor de pdf
- pip install -U streamlit             // visual para o chat
- pip install -U python-dotenv
- pip install -U requests
- pip install -U rich                  // visual para o terminal
```

# â–¶ï¸ Ativando e desativando o ambiente
Utilize os comandos no terminal, dentro da pasta do projeto.

### Ativar
```text
.venv\Scripts\activate
```
### Desativar
```text
deactivate
```
# ğŸ“ ObservaÃ§Ãµes
- O projeto mostra a evoluÃ§Ã£o do terminal ao frontend web.
- Cada aula traz um passo incremental na construÃ§Ã£o do chatbot.
- Quando hÃ¡ PDF carregado, o conteÃºdo Ã© usado como contexto para as respostas.
- A versÃ£o final inclui memÃ³ria de conversa, permitindo continuidade entre interaÃ§Ãµes.
